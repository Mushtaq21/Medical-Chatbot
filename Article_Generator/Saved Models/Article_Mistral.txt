import random
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Mistral model and tokenizer
model_name = "mistralai/Mistral-7B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Set padding token
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = model.config.eos_token_id

# Predefined random topics
topics = [
    "The rise of electric vehicles",
    "Ancient civilizations and their contributions",
    "The benefits of mindfulness and meditation",
    "How cryptocurrencies are changing the economy",
    "The effects of music on the human brain"
]

# Pick a random topic
random_topic = random.choice(topics)
print(f"Generating an article on: {random_topic}\n")

# Function to generate an article
def generate_article(prompt, max_length=300):
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)

    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_length=max_length,
        num_return_sequences=1,
        no_repeat_ngram_size=2,
        top_p=0.95,
        top_k=50,
        temperature=0.7,
        do_sample=True,
        repetition_penalty=1.2,
        pad_token_id=model.config.eos_token_id
    )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Generate and print the article
prompt = f"Write an informative article about {random_topic}."
article = generate_article(prompt)
print("Generated Article:\n", article)
