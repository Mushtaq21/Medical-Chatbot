import random
from transformers import GPTNeoForCausalLM, GPT2Tokenizer
import torch

# Load pre-trained model and tokenizer
model_name = "EleutherAI/gpt-neo-125M"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPTNeoForCausalLM.from_pretrained(model_name)

# Set pad_token_id to eos_token_id to handle padding issues
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = model.config.eos_token_id

# Predefined list of random topics
topics = [
    "The importance of space exploration",
    "The impact of climate change on wildlife",
    "The history of the internet",
    "How do vaccines work?",
    "The psychology behind dreams",
    "The rise of electric vehicles",
    "Ancient civilizations and their contributions",
    "The benefits of mindfulness and meditation",
    "How cryptocurrencies are changing the economy",
    "The effects of music on the human brain"
]

# Select a random topic
random_topic = random.choice(topics)
print(f"Generating an article on: {random_topic}")

# Function to generate an article on a random topic
def generate_article(prompt, max_length=300):
    # Encode the input prompt to tensor
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)

    # Generate output (optimized parameters)
    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_length=max_length,
        num_return_sequences=1,
        no_repeat_ngram_size=2,
        top_p=0.95,
        top_k=50,
        temperature=0.7,
        do_sample=True,
        repetition_penalty=1.2,
        pad_token_id=model.config.eos_token_id
    )

    # Decode the generated text and return it
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_text

# Generate and print the article
prompt = f"Write an informative article about {random_topic}."
generated_article = generate_article(prompt)
print("\nGenerated Article:\n", generated_article)
