{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import random\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32  # Use FP16 on GPU for speed\n",
        "\n",
        "# List of optimized open-source models\n",
        "models = {\n",
        "    \"GPT-Neo\": \"EleutherAI/gpt-neo-125M\",  # Lightweight model\n",
        "    \"Mistral\": \"mistralai/Mistral-7B-Instruct-v0.1\",  # Efficient model\n",
        "    \"Falcon\": \"tiiuae/falcon-7b\"  # Optimized model\n",
        "}\n",
        "\n",
        "# Predefined list of random topics\n",
        "topics = [\n",
        "    \"The importance of space exploration\",\n",
        "    \"The impact of climate change on wildlife\",\n",
        "    \"The history of the internet\",\n",
        "    \"How do vaccines work?\",\n",
        "    \"The psychology behind dreams\",\n",
        "    \"The rise of electric vehicles\",\n",
        "    \"Ancient civilizations and their contributions\",\n",
        "    \"The benefits of mindfulness and meditation\",\n",
        "    \"How cryptocurrencies are changing the economy\",\n",
        "    \"The effects of music on the human brain\"\n",
        "]\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Article Generator using Open-Source LLMs\")\n",
        "st.sidebar.header(\"Settings\")\n",
        "\n",
        "# Select model\n",
        "model_name = st.sidebar.selectbox(\"Choose a model\", list(models.keys()))\n",
        "random_topic = st.sidebar.selectbox(\"Choose a topic\", topics)\n",
        "\n",
        "@st.cache_resource()\n",
        "def load_model(model_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(models[model_name])\n",
        "    model = AutoModelForCausalLM.from_pretrained(models[model_name], torch_dtype=torch_dtype).to(device)\n",
        "    \n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = model.config.eos_token_id\n",
        "    return model, tokenizer\n",
        "\n",
        "# Load model once\n",
        "model, tokenizer = load_model(model_name)\n",
        "\n",
        "# Function to generate an article\n",
        "def generate_article(model, tokenizer, prompt, max_length=150):  # Reduced max_length for faster generation\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        top_p=0.9,\n",
        "        top_k=20,  # Reduced top_k for speed\n",
        "        temperature=0.8,  # Balanced creativity & speed\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.1,\n",
        "        pad_token_id=model.config.eos_token_id\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "if st.button(\"Generate Article\"):\n",
        "    st.write(f\"### Generating an article on: {random_topic} using {model_name}\")\n",
        "    prompt = f\"Write an informative article about {random_topic}.\"\n",
        "    article = generate_article(model, tokenizer, prompt)\n",
        "    st.write(article)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}